## 딥다이브는 맨 아래에 있습니다!  
# 5. Support Vector Machine  

## 5-1. 선형 SVM Classifiation
기본 : Large Margin Classification  
 - 클래스 사이에 폭이 가장 넓은 도로를 찾는 것
![image](https://github.com/user-attachments/assets/93e4c855-6979-4f04-9bd7-2c0c0341b69a)
가장 경계에 걸쳐 있는, 분류의 기준이 되는 샘플 : Support Vector  
Supprt Vector 간의 거리 : Margin  

Hard Margin의 단점 : 이상치에 민감하다. 어떤 예외도 있으면 안되기 때문  
![image](https://github.com/user-attachments/assets/d9130e0e-909a-4bab-bcf0-9fd5c3c73649)  

Soft Margin : 약간의 예외를 허용하는 것.  
C : 규제 Hyperparameter  
![image](https://github.com/user-attachments/assets/79781483-50ea-4944-b92b-245c1c6b400e)  

## 5-2. 비선형 SVM Classification  
 - 비선형 dataset에 특성을 추가해서 선형 분류가 가능하게 하는 것.
![image](https://github.com/user-attachments/assets/2d2240b2-ba21-4750-9245-325b49629406)  
![image](https://github.com/user-attachments/assets/cc01512a-bdb7-4c5a-9700-d1b32a1a0c14)   

###5-2.2 유사도 특성  
유사도 함수 : 각 sample이 랜드마크와 얼마나 닮았는 지 측정함으로써 새로운 좌표축을 설정하는 것.  
![image](https://github.com/user-attachments/assets/9a228d24-33df-4524-886d-f99118497e19)  

## 5-3. SVM 회귀(SVR)  
SVM 분류 : 클래스 간의 도로폭이 최대가 되도록  
SVM 회귀 : 도로 안에 가능한 많은 샘플이 들어가도록  
![image](https://github.com/user-attachments/assets/6b3d7cd8-96e9-4c6c-acb0-6fc1286ee6b9)  

## 5-4. SVM이론  
![image](https://github.com/user-attachments/assets/f6bed6fa-2e0f-488a-8c46-2e186a9d606c)  
선형 SVM 분류기로 예측을 하려면 가중치 벡터 W와 bias b를 찾아야 한다.  
![image](https://github.com/user-attachments/assets/0a249d07-9c04-429b-8fb3-21af0da2abe3)  
가중치 벡터 W에 따라 그래프의 기울기(너비)가 바뀌고, 마진도 바뀌는 걸 알 수 있다.  
![image](https://github.com/user-attachments/assets/17808263-cce2-49d7-996b-0f9e61b74232)  
![image](https://github.com/user-attachments/assets/b29ce02e-795b-43ea-a48d-1fb22a530e46)  
![image](https://github.com/user-attachments/assets/9aa8998b-4134-4764-84cf-2b5206a66954)



# 6. Decision Tree  
![스크린샷 2025-02-04 185148](https://github.com/user-attachments/assets/62061d9b-c898-439a-87fa-e0f94ea7c5f9)  
비교적 간단하다.  
 - 지니 계수  
![image](https://github.com/user-attachments/assets/b51c1bf2-b0f9-4bbb-bf81-6cbefecad464)

 - 엔트로피 불순도
![image](https://github.com/user-attachments/assets/28b369eb-9c85-4cf3-ad35-bdc996bc4d9f)
ex) 모든 sample의 class가 동일하면 entrophy는 0이 된다.


![image](https://github.com/user-attachments/assets/294ef41f-a110-4656-85bc-54ff231ce426)

## 6-3 클래스 확률 추정  
 -- 한 sample이 클라스 k에 속할 확률  
1. 해당 sample의 leaf node 찾기
2. 해당 node에 각 class인 sample의 비율 찾기
3. 그 중 제일 높은 확률에 속한 class

## 6-4.CART 알고리즘  
: 트리를 어떤 기준으로 나눌까? 어떤 특성의 어떤 임계점을 기준으로 나눌까?  
아래 비용함수가 최소가 되는 것으로 나눔  
![image](https://github.com/user-attachments/assets/377a2ed0-24d2-4cfa-b8a2-cf79693a5ff5)  

 - Decision Tree를 탐색하는 시간복잡도 : O(log_2)(n)

### 규제 매개변수 :  
![image](https://github.com/user-attachments/assets/f813a47c-e269-4680-8ba1-ca117f3d36fb)  
![image](https://github.com/user-attachments/assets/8b564a0c-59d5-4595-b50c-d8481f0ac78d)  

## 6-8. Decision Tree를 이용한 회귀  
: 기존의 트리는 class를 예측하는 대신 회귀는 값을 예측한다
![image](https://github.com/user-attachments/assets/b578bf24-428b-47e7-8006-fd4b0789ada9)  
![image](https://github.com/user-attachments/assets/4e38f8fd-e899-4d61-a94a-bb09122bf836)  
![image](https://github.com/user-attachments/assets/6dadb8ab-4df9-47da-b5fb-1161df0ad80f)  
![image](https://github.com/user-attachments/assets/d87a0649-d331-4b61-9640-32f08c585f54)  

## 6-9 축 방향에 대한 민감성  
결정 트리의 경계는 모두 축에 수직입니다.  
그래서 데이터의 방향에 민감합니다.  
![image](https://github.com/user-attachments/assets/970f6ea3-1814-4354-b6fa-9cdc14ff80a7)  







***
## Deep Dive
