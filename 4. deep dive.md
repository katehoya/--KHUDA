## 1. Lidge, Lasso 회귀  
 - 어떤 경우에 릿지 회귀가 유리하며 어떤 경우에 라쏘 회귀가 유리한가?
     Lasso : 가중치가 0이되는 feature가 존재한다. 그래서 선형 회귀에 적용하면 원한는 몇몇 feature만을 선택할 수 있는 효과가 있다.
     Ridge : 한 feature를 완전히 제외한다기 보다는 모든 feature들이 조금씩 다 필요할 때. feature들이 서로 상관관계가 있는 경우

 - 데이터의 크기와 차원이 커질 경우 둘의 성능과 효율성은 어떻게 변화할까?
     Ridge : l2-norm을 사용하기 때문에 차원이 좀 높아져도 효율적으로 regularize할 수 있다. 그러나 모든 feature를 다 사용하기 때문에 계산이 느리다.
     Lasso : 원하는 feature를 선택할 수 있기 때문에 차원이 높아져도 계산이 효율적으로 작동할 가능성이 높다. 그러나 상관관계가 높은 feature들이 있을 경우에는 중요한 데이터가 삭제될 수도 있다.
   
 - 엘라스틱넷은 둘의 어떠한 장점과 특징을 각각 가져가는가?
     둘 다 적합하지 않을 때 엘라스틱넷을 사용한다. 엘라스틱넷은 ridge, lasso의 항을 모두 갖고 있기 때문에 상호보완적으로 사용가능하기 때문이다.
     Lasso의 원하지 않는 feature를 삭제할 수도 있지만 동시에 ridge 항으로 인해 해당 feature가 완전히 사라지지는 않고, 그 가중치가 줄어들어 모든 특성을 유지하되 잠재적인 상관관계를 해치지 않는다.
     
## 2. 미니배치 경사하강법의 배치크기 
- 배치 크기가 모델 학습에 미치는 영향은 무엇일까?
  일단 GPU 메모리의 크기 때문에 너무 큰 배치 사이즈를 넣지 못 할 수도 있다.
  배치 사이즈가 작아지면 업데이트를 자주 하게 되지만 학습 noise가 커진다. 그래서  regularization 효과가 있다
  그러나 너무 작은 배치 사이즈는 오히려 underfitting 위험성이 있다.
  작은 배치 사이즈는 flat minimum(일반화 성능이 좋은 최솟값)을 갖는 경향이 있고 큰 배치 사이즈는 sharp minimum(overfitting)을 잘 찾는 경향이 있다.
- 어떤 경우에 큰 배치가 유리하고 어떤 경우에 작은 배치가 유리한가?
  큰 배치는 수렴 속도가 빠르고, noise에 둔감하고, GPU, TPU의 병렬 연산 이점을 살릴 수 있다는 장점이 있다. 또한 매 UPDATE마다 작은 배치보다 덜 급격히 움직이기 때문에 안정적인 학습이 가능하다.
  작은 배치는 Generalization성능이 좋으며 flat minimum을 찾기 때문에 non-convex 문제를 풀 때 좋다. 또한 실시간 시스템에서 매번 들어오는 데이터를 작은 batch로 학습하는 게 유리할 수도 있다.
- 여러 배치 크기를 일일이 실험해보는 것 없이 효율적으로 최적화된 배치 크기를 찾는 방법은 없을까?
  Baysian Optimization
  hyper parameter(batch size, learning rate..)등을 처음에 몇 개를 test해보고 각 hyper parameter에 대한 정확도를 측정함. 그리고 이를 이용해 hyper parameter와 성능과의 관계를 찾아서 최적의 배치 사이즈를 찾음. -> batch size 찾는 데 비용이 적게 듬, 다양한 hyper parameter에 적용가능
  
## 3. 소프트맥스와 손실 함수
- 손실 함수에는 어떤 것들이 있을까?
  MSE, Binary Cross Entrophy, Categorical Cross Entrophy
- 크로스 엔트로피 손실 함수의 직관적인 해석은?
  정답 클래스에 대한 예측 확률 분포의 -log.
- 왜 소프트맥스와 크로스 엔트로피를 결합하는 것이 최적화에 유리한가?
  Softmax로 확률을 만들고, Cross Entrophy로 측정하면 간결한 미분 형태를 얻어 학습이 빠르고 안정적이며 예측 분포를 실제 정답과 비교할 때 수학적으로도 간단하고 안정적인 gradient를 제공하기 때문
