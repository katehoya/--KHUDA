# 4.1 Linear Regression

기본 모델 : ![image](https://github.com/user-attachments/assets/7fac37d0-04bf-4cae-8f11-29aa41d7361a)  
벡터 형태 : ![image](https://github.com/user-attachments/assets/352ad60f-1cc3-4581-a26a-a5602f2cf3a3)  
Cost Function으로는 MSE를 사용 ![image](https://github.com/user-attachments/assets/a0d0e750-6d47-483d-9f7a-659df0656033)  
==> Cost function을 최소화 하는 parameter를 찾아야 함
  방법 1 : 정규 방정식 이용.&nbsp;&nbsp;&nbsp;&nbsp;![image](https://github.com/user-attachments/assets/0d635ea7-7f7e-4c84-8cc1-505041632948)  
장점 : 간단하다  
단점 : X^T X를 계산하는 게 어렵다. Time Complexity가 O(n^3)나 된다.
  방법 2 : Gradient Descent 이용  

# 4.2 Gradient Descent

Cost function이 작아지도록 theta 수정.  
![image](https://github.com/user-attachments/assets/d0122cb8-bdd9-4fad-89d8-4b58e0ba0f08)  
Step의 크기(Learning Rate) : hyperparameter로써 정해줘야 함.  
Learning rate 너무 작으면 - 수렴 속도가 너무 느려짐  ![image](https://github.com/user-attachments/assets/fe0b7aee-3c3a-4f02-b229-05bbc052d355)  
Learning rate 너무 크면 - minima를 벗어날 수도 있음  ![image](https://github.com/user-attachments/assets/5713010a-70b8-4e20-a3fc-1411027404a6)  

Gradient Descent의 단점 : local minima와 global minima가 불일치 할 경우 global minima를 찾는 것이 복잡해질 수도 있다. 
![image](https://github.com/user-attachments/assets/66bba130-9edc-4df2-97ae-4737bfcb30ab)  
그래서 처음에는 Learning rate를 큰 값으로 설정하고(for Global minima를 찾기 위해) 나중으로 갈 수록 점점 Learning rate를 줄여야 함(for local minima를 찾기 위해.  
그러나 Cost Function으로 MSE를 쓴다면 MSE가 convex function이 되어 global minima == local minima 가 된다.  

minima를 찾아서 가는 법 : ![image](https://github.com/user-attachments/assets/8e488b3c-bcd2-4748-8914-24d56fbb9940)  
![image](https://github.com/user-attachments/assets/44a594da-10b1-4cb2-a540-2772ff14c998)  
![image](https://github.com/user-attachments/assets/8c3dcf79-e3c6-40e5-8bf5-fa5ca66b3ca5)  
![image](https://github.com/user-attachments/assets/1f978df0-9803-487c-bc42-2f89f314d390)  
![image](https://github.com/user-attachments/assets/6edf43a4-c822-4a1e-bf01-a874dd270dfc)  

이렇게 각 step마다 점점 minima를 찾아서 가게 된다.  
각 step의 발걸음을 언제 딛을 것이냐에 따라 Batch Gradient Descent, Stochastic Gradient Descent로 나눌 수 있다.  
Batch : 전체 training set을 훈련하고 난 뒤에 step.  
Stochastic : 각 sample을 훈련하고 난 뒤에 step.  
Mini-batch : 임의의 작은 sample set( named mini-batch)를 훈련하고 난 뒤에 step을 감.  
![image](https://github.com/user-attachments/assets/eabf1004-a06e-445e-beb2-28cfc44fb012)  

![image](https://github.com/user-attachments/assets/53ae6247-80ae-46dc-a977-6804a3aa2175)  


++추가하기

++ Feature Scaling을 해줘야 한다  
![image](https://github.com/user-attachments/assets/1256d3ba-9b07-4d04-b2f4-5c6ff8531701)  

# 4.3 Polynomial Regression  


# 4.4 Learning curve  
Learning curve : Overfitting, Underfitting 여부를 확인할 수 있는 근거가 됨  


![image](https://github.com/user-attachments/assets/1328981a-c339-410f-9555-43dcb8690ec9)  
-> Underfitting이다.  
training set, test set의 차이가 크지 않으므로 overfitting은 아닌데,  
오차가 너무 크기 때문에 underfitting이 의심된다. 또한 training set의 크기가 커져도 오차가 줄어들지 않는 걸 보면 모델의 표현력이 부족하다고 볼 수 있다.

![image](https://github.com/user-attachments/assets/83d8c6fe-697c-49c8-9ab9-189347c51547)  
-> overfitting이다.
training set의 오차가 test보다 너무 낮다.  

# 4.5 Regularization  
MSE에 규제항을 추가함으로써 가중치를 작게 유지.  
![image](https://github.com/user-attachments/assets/6382c807-f8fe-41fd-baf9-9b44c0820c1c)  
a : model을 얼마나 많이 규제할 지.

##  Ridge 회귀


# 4.6 Logistic Regression
