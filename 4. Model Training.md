![image](https://github.com/user-attachments/assets/f254c2cd-07c6-4af7-899e-448b3ce6a806)# 4.1 Linear Regression

기본 모델 : ![image](https://github.com/user-attachments/assets/7fac37d0-04bf-4cae-8f11-29aa41d7361a)  


벡터 형태 : ![image](https://github.com/user-attachments/assets/352ad60f-1cc3-4581-a26a-a5602f2cf3a3)  
Cost Function으로는 MSE를 사용 ![image](https://github.com/user-attachments/assets/a0d0e750-6d47-483d-9f7a-659df0656033)  
==> Cost function을 최소화 하는 parameter를 찾아야 함
  방법 1 : 정규 방정식 이용.&nbsp;&nbsp;&nbsp;&nbsp;![image](https://github.com/user-attachments/assets/0d635ea7-7f7e-4c84-8cc1-505041632948)  
장점 : 간단하다  
단점 : X^T X를 계산하는 게 어렵다. Time Complexity가 O(n^3)나 된다.
  방법 2 : Gradient Descent 이용  

# 4.2 Gradient Descent

Cost function이 작아지도록 theta 수정.  
![image](https://github.com/user-attachments/assets/d0122cb8-bdd9-4fad-89d8-4b58e0ba0f08)  
Step의 크기(Learning Rate) : hyperparameter로써 정해줘야 함.  
Learning rate 너무 작으면 - 수렴 속도가 너무 느려짐  ![image](https://github.com/user-attachments/assets/fe0b7aee-3c3a-4f02-b229-05bbc052d355)  
Learning rate 너무 크면 - minima를 벗어날 수도 있음  ![image](https://github.com/user-attachments/assets/5713010a-70b8-4e20-a3fc-1411027404a6)  

Gradient Descent의 단점 : local minima와 global minima가 불일치 할 경우 global minima를 찾는 것이 복잡해질 수도 있다. 
![image](https://github.com/user-attachments/assets/66bba130-9edc-4df2-97ae-4737bfcb30ab)  
그래서 처음에는 Learning rate를 큰 값으로 설정하고(for Global minima를 찾기 위해) 나중으로 갈 수록 점점 Learning rate를 줄여야 함(for local minima를 찾기 위해.  
그러나 Cost Function으로 MSE를 쓴다면 MSE가 convex function이 되어 global minima == local minima 가 된다.  

minima를 찾아서 가는 법 : ![image](https://github.com/user-attachments/assets/8e488b3c-bcd2-4748-8914-24d56fbb9940)  
![image](https://github.com/user-attachments/assets/44a594da-10b1-4cb2-a540-2772ff14c998)  
![image](https://github.com/user-attachments/assets/8c3dcf79-e3c6-40e5-8bf5-fa5ca66b3ca5)  
![image](https://github.com/user-attachments/assets/1f978df0-9803-487c-bc42-2f89f314d390)  
![image](https://github.com/user-attachments/assets/6edf43a4-c822-4a1e-bf01-a874dd270dfc)  

이렇게 각 step마다 점점 minima를 찾아서 가게 된다.  
각 step의 발걸음을 언제 딛을 것이냐에 따라 Batch Gradient Descent, Stochastic Gradient Descent로 나눌 수 있다.  
Batch : 전체 training set을 훈련하고 난 뒤에 step.  
Stochastic : 각 sample을 훈련하고 난 뒤에 step.  
Mini-batch : 임의의 작은 sample set( named mini-batch)를 훈련하고 난 뒤에 step을 감.  
![image](https://github.com/user-attachments/assets/eabf1004-a06e-445e-beb2-28cfc44fb012)  

![image](https://github.com/user-attachments/assets/53ae6247-80ae-46dc-a977-6804a3aa2175)  


++추가하기

++ Feature Scaling을 해줘야 한다  
![image](https://github.com/user-attachments/assets/1256d3ba-9b07-4d04-b2f4-5c6ff8531701)  

# 4.3 Polynomial Regression  


# 4.4 Learning curve  
Learning curve : Overfitting, Underfitting 여부를 확인할 수 있는 근거가 됨  


![image](https://github.com/user-attachments/assets/1328981a-c339-410f-9555-43dcb8690ec9)  
-> Underfitting이다.  
training set, test set의 차이가 크지 않으므로 overfitting은 아닌데,  
오차가 너무 크기 때문에 underfitting이 의심된다. 또한 training set의 크기가 커져도 오차가 줄어들지 않는 걸 보면 모델의 표현력이 부족하다고 볼 수 있다.

![image](https://github.com/user-attachments/assets/83d8c6fe-697c-49c8-9ab9-189347c51547)  
-> overfitting이다.
training set의 오차가 test보다 너무 낮다.  

# 4.5 Regularization  
MSE에 규제항을 추가함으로써 가중치를 작게 유지.(가중치 감쇠 라고도 한다)  
![image](https://github.com/user-attachments/assets/4d030630-340e-4394-bcc6-294942429c5e)  
람다 : model을 얼마나 많이 규제할 지.  
규제항으로 무엇을 사용할 것인가에 따라 Ridge, Lasso regression 있다.
![image](https://github.com/user-attachments/assets/e6aa8c0e-8456-4309-bde9-e2a24b20b847)  
##  Ridge regression  
![image](https://github.com/user-attachments/assets/2da64275-87af-4c05-b92f-72cc7cd67d26)  
![image](https://github.com/user-attachments/assets/31795d0a-3e04-4c9d-990e-af752fd61327)  
![image](https://github.com/user-attachments/assets/95e626e7-5a2e-41c7-bedc-d41c9479c12b)  
![image](https://github.com/user-attachments/assets/9891a920-c7c3-4af9-b929-39b524bc38cc)  
![image](https://github.com/user-attachments/assets/e056f9cf-b04e-488b-89c6-3500b25c0689)  

## Lasso regression  
![image](https://github.com/user-attachments/assets/29ddcfa8-e9c4-4b13-bfe4-c5ea21b076f8)  
![image](https://github.com/user-attachments/assets/772b022f-249b-4c0b-b4ea-96fd7174ebaf)  
![image](https://github.com/user-attachments/assets/762db6dc-1f16-42ea-b17b-6733b93c1b4b)  
![image](https://github.com/user-attachments/assets/84a152cc-da1a-41c4-a183-3c296f63a862)  

![image](https://github.com/user-attachments/assets/7195a393-e0cb-4a6f-88e3-6d73f744124b)

## Elastic net regression  
Lidge, Lasso를 절충한 모델이다. 일반적으로 가장 좋다  
![image](https://github.com/user-attachments/assets/2c2012dc-4e74-4533-8d8d-8c7486bde166)  
--> 규제항이 단순히 Lidge, Lasso의 그것을 더한 모습.  

## 조기 종료  
별다른 규제항을 추가하지 않고, 단순히 training set, test set 간의 정확도가 차이가 나는 시점(overfitting이 시작하는 시점)에 학습을 중단시킴.  
![image](https://github.com/user-attachments/assets/b957eafa-28b2-4e7b-b1df-59f97b454f92)  

# 4.6 Logistic Regression  
 - Activation function으로서 logistic fuction을 사용하는 것.
 - binary classification에서, sample이 positive class에 속할 확률을 구할 수 있다.  
![image](https://github.com/user-attachments/assets/73bab3be-9f69-4ff9-b764-a127265669ba)  
![image](https://github.com/user-attachments/assets/680d03da-c46d-45ae-9118-0514d6ff18b5)

ex)  ![image](https://github.com/user-attachments/assets/49450324-5b84-4c5a-babc-b62c34c92b58)  
![image](https://github.com/user-attachments/assets/cd9d59eb-39a4-4c51-a510-3affc3d0011f)  

## Softmax Regression   
 - Activation function으로서 softmax function을 사용하는 것.  
 - multiclass classification에서 sample x가 각 class에 속할 확률을 알 수 있다.
![image](https://github.com/user-attachments/assets/b8452701-4655-47cc-99fa-65489f34863c)  
 - 그 중 가장 확률이 높은 class를 선택하는 것이다.  
![image](https://github.com/user-attachments/assets/4be92deb-87e6-4bc7-afc0-5b75d3477f41)

ex)  ![image](https://github.com/user-attachments/assets/37717b94-8902-494b-b55d-4380ec19e824)  



